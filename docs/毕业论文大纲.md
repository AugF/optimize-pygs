---
title: 组会报告_210107
author: wangyunpan
---

# 基于GPU的大规模图神经网络计算性能瓶颈分析与解决方法研究
[toc]


## 1. 绪论

### 1.1 研究背景

图神经网络(GNN)是一类基于深度学习的处理图域信息的方法，它通过将图广播操作和深度学习算法结合，可以让图的结构信息和顶点属性都参与到学习中，在顶点分类、图分类、链接预测等应用中表现出良好的效果和可解释性，已成为一种广泛应用的图分析方法。

### 1.2 国内外研究现状


### 1.3 现有工作不足

### 1.4 本文的研究内容和主要贡献点

### 1.5 本文的组织结构

## 2. 相关背景知识

### 2.1 图神经网络概述

### 2.2 图神经网络系统

## 3. 图神经网络计算的性能瓶颈


## 4. 基于单机单GPU的训练时间优化

Motivation:
1. 不管是分布式，还是单机多GPU, 如何解决单机单GPU始终是其核心的问题
2. 现有图神经网络训练流程中，训练阶段流程为Forward, Backward, Evaluation, 没有充分利用到GPU的资源和CPU的资源
3. 传统DL中，虽然 

目标：
优化图神经网络训练的执行流程，从而提高训练时间。

配置：

在这里对深度学习训练阶段采用的两种训练方式进行设计：
1. 固定轮数: (正则化项)
每个epoch
Forward, backward, evaluation
> 在训练结束后，只要内存够用，Evaluation可以并行化运行

2. early_stopping机制; (提前停止) 
前x个epoch
forward, backward
每隔一定的epoch
Forward, backward, evaluation
> 一个潜在的问题：原本epoch结束后，执行evaluation即可终止训练，但是现在的结束条件是等GPU的Evaluation或CPU端的Evaluation结束才行
> 一个策略：自动感知是否已经到了收敛阶段
> 涉及到多个问题：cpu和gpu数据传输时间， 以及存文件和读文件的时间

> 文件系统的快速访问这里也需要进行思考如何做
> 数据通信这块的时间减不下来

### 全数据训练下的优化

1. 固定轮数 

step1 预热阶段：
统计Forward和Backward在GPU上的总时间T_train, 峰值内存M_train;
统计Evaluation在GPU上运行的总时间T_evaluation, 峰值内存M_evaluation
统计Evaluation在CPU上运行的总时间T_evaluation_cpu, 峰值内存M_evaluation_cpu

step2 预执行
在GPU上预先执行Forward和Backward x轮

step3 异步evaluation
如果GPU内存支持，将GPU保存到另一块GPU, 然后异步evaluation。
否则，在CPU端进行异步训练evaluation

step4 训练结束，汇报最终结果

2. early_stopping机制:
在固定轮数的基础上：
evaluation的时机不一样，其次在每次Evaluation运行结束后，做模型终止条件的判断
> 这里在哪个时机进行判断也是一个有一定讲究的事情; Evaluation结束时，可以使得在有限时间内更新更多次，一般来说，验证集规模远小于训练集的

### sample-based训练下的优化

sample和training的overlap

sample和training的overlap
> 问题：先考虑全部将数据放到GPU

## 5. 基于GPU的内存稳定

## 6. 实验性能评估

### 

## 7. 总结与展望

### 7.1 本文工作总结
### 7.2 进一步工作

## 8. 参考文献

## 附录

## 致谢
